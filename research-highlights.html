<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <title>Research Highlights</title>
    <style>
        body {
            font-family: Arial, sans-serif;
        }
        .research-outcomes {
            width: 100%;
            border-collapse: collapse;
        }
        .research-outcomes th, .research-outcomes td {
            padding: 10px;
            border: 1px solid #ddd;
            text-align: left;
        }
        .research-outcomes th {
            background-color: #f4f4f4;
        }
        .highlight {
            background-color: #ffffcc;
        }
        .image-container {
            width: 150px;
        }
        .image-container img {
            width: 100%;
            height: auto;
        }
        .publication-info {
            padding-left: 20px;
        }
        /* Optional: Styling for the hidden content */
        .bibtex {
                display: none; /* Initially hide the content */
                margin-top: 10px; /* Add some spacing */
                padding: 10px;
                background-color: #f0f0f0;
                border: 1px solid #ccc;
        }
    </style>
</head>
<body>
    <h1>Recent Research Highlights (Selected)</h1>
    <table class="research-outcomes">
        <tr class="highlight">
            <td class="image-container">
                <img src="./research/motion-agent.jpg" alt="Motion-Agent">
            </td>
            <td class="publication-info">
                <h3>Motion-Agent: A Conversational Framework for Human Motion Generation with LLMs, ICLR 2024</h3>
                Qi Wu*, Yubo Zhao*, Yifan Wang, Xinhang Liu, Yu-Wing Tai, Chi-Keung Tang (* denotes equal contribution)<br>

                <FONT color=red>Motion-Agent is a new framework that generates and edits human motion via conversation.</FONT><br>
                [<a href="https://arxiv.org/abs/2405.17013" target="_blank">Paper</a>] [<a href="https://github.com/szqwu/Motion-Agent" target="_blank">Project</a>] [<a href="https://youtu.be/7fObcfrdUO0" target="_blank">Youtube</a>]
                
                <iframe src="https://ghbtns.com/github-btn.html?user=szqwu&repo=Motion-Agent&type=star&count=true&v=2" frameborder="0" scrolling="0" width="170" height="20"></iframe>
                
                </p>
            </td>
        </tr>
        <tr class="highlight">
            <td class="image-container">
                <img src="./research/chatcam.png" alt="ChatCam">
            </td>
            <td class="publication-info">
                <h3>ChatCam: Empowering Camera Control through Conversational AI, NeurIPS 2024</h3>
                Xinhang Liu, Yu-Wing Tai, Chi-Keung Tang<br>

                <FONT color=red>An AI-Agent to control camera movements, enabling precise cinematographic workflows through natural language.</FONT><br>
                [<a href="https://arxiv.org/abs/2409.17331" target="_blank">Paper</a>] [<a href="https://github.com/DarlingHang/ChatCam" target="_blank">Project</a>] 
                
                <iframe src="https://ghbtns.com/github-btn.html?user=DarlingHang&repo=ChatCam&type=star&count=true&v=2" frameborder="0" scrolling="0" width="170" height="20"></iframe>
                
                </p>
            </td>
        </tr>
        <tr class="highlight">
            <td class="image-container">
                <img src="./research/deceptive_eccv24.jpg" alt="Deceptive-NeRF/3DGS">
            </td>
            <td class="publication-info">
                <h3>Deceptive-NeRF/3DGS: Diffusion-Generated Pseudo-Observations for High-Quality Sparse-View Reconstruction, ECCV 2024</h3>
                Xinhang Liu, Jiaben Chen, Shiu-hong Kao, Yu-Wing Tai, Chi-Keung Tang<br>

                <FONT color=red>Very High Quality 3D Reconstruction from Sparse Inputs</FONT><br>
                [<a href="https://arxiv.org/abs/2305.15171" target="_blank">Paper</a>] [<a href="https://github.com/DarlingHang/deceptive-nerf" target="_blank">Project</a>] 
                
                <iframe src="https://ghbtns.com/github-btn.html?user=DarlingHang&repo=deceptive-nerf&type=star&count=true&v=2" frameborder="0" scrolling="0" width="170" height="20"></iframe>
                
                </p>
            </td>
        </tr>
        <tr class="highlight">
            <td class="image-container">
                <img src="./research/dragvideo-eccv24.gif" alt="DragVideo">
            </td>
            <td class="publication-info">
                <h3>DragVideo: Interactive Drag-style Video Editing, ECCV 2024</h3>
                Yufan Deng*, Ruida Wang*, Yuhao Zhang*, Yu-Wing Tai, Chi-Keung Tang
                (* denotes equal contribution)<br>

                <FONT color=red>DragVideo is one of the first drag-style editing methods for precise, and consistent video editing.</FONT><br>
                [<a href="https://arxiv.org/abs/2312.02216" target="_blank">Paper</a>] [<a href="https://github.com/RickySkywalker/DragVideo-Official" target="_blank">Project</a>] 
                
                <iframe src="https://ghbtns.com/github-btn.html?user=RickySkywalker&repo=DragVideo-Official&type=star&count=true&v=2" frameborder="0" scrolling="0" width="170" height="20"></iframe>
                
                </p>
            </td>
        </tr>
        <tr class="highlight">
            <td class="image-container">
                <img src="./research/GoldDistillation.png" alt="Gold Distillation">
            </td>
            <td class="publication-info">
                <h3>Distill Gold from Massive Ores: Efficient Dataset Distillation via Critical Samples Selection, ECCV 2024</h3>
                Yue Xu, Yong-Lu Li, Kaitong Cui, Ziyu Wang, Cewu Lu, Yu-Wing Tai, Chi Keung Tang<br>

                <FONT color=red>Efficient dataset distillation for very large datasets.</FONT><br>
                [<a href="https://arxiv.org/abs/2305.18381" target="_blank">Paper</a>] [<a href="https://github.com/silicx/GoldFromOres-BiLP" target="_blank">Project</a>] 
                
                <iframe src="https://ghbtns.com/github-btn.html?user=silicx&repo=GoldFromOres-BiLP&type=star&count=true&v=2" frameborder="0" scrolling="0" width="170" height="20"></iframe>
                
                </p>
            </td>
        </tr>
        <tr class="highlight">
            <td class="image-container">
                <img src="./research/gearnerf-cvpr24.jpeg" alt="GEAR-NERF">
            </td>
            <td class="publication-info">
                <h3>Gear-NeRF: Free-Viewpoint Rendering and Tracking with Motion-aware Spatio-Temporal Sampling, CVPR 2024</h3>
                Xinhang Liu, Yu-Wing Tai, Chi-Keung Tang, Pedro Miraldo, Suhas Lohit, Moitreya Chatterjee<br>

                <FONT color=red>CVPR Highlight, 2.8% of 11532</FONT><br>
                [<a href="https://arxiv.org/abs/2406.03723" target="_blank">Paper</a>] [<a href="https://github.com/merlresearch/Gear-NeRF" target="_blank">Project</a>] [<a href="https://youtu.be/3Pg92mfENds?si=S-jbS2Hm6Wo2Dj7P" target="_blank">Youtube</a>]
                
                <iframe src="https://ghbtns.com/github-btn.html?user=merlresearch&repo=Gear-NeRF&type=star&count=true&v=2" frameborder="0" scrolling="0" width="170" height="20"></iframe>
                
                </p>
            </td>
        </tr>
        <tr class="highlight">
            <td class="image-container">
                <img src="./research/SANeRF-HQ.png" alt="SANeRF-HQ">
            </td>
            <td class="publication-info">
                <h3>SANeRF-HQ: Segment Anything for NeRF in High Quality, CVPR 2024</h3>
                Yichen Liu, Benran Hu, Chi-Keung Tang, Yu-Wing Tai<br>

                <FONT color=red>Segment Anything in High Quality for NeRF</FONT><br>
                [<a href="https://arxiv.org/abs/2312.01531" target="_blank">Paper</a>] [<a href="https://github.com/lyclyc52/SANeRF-HQ" target="_blank">Project</a>] [<a href="https://youtu.be/f4HmH0OsIuY?si=069Avhg4xaK0uYfm" target="_blank">Youtube</a>]
                
                <iframe src="https://ghbtns.com/github-btn.html?user=lyclyc52&repo=SANeRF-HQ&type=star&count=true&v=2" frameborder="0" scrolling="0" width="170" height="20"></iframe>
                
                </p>
            </td>
        </tr>
        <tr class="highlight">
            <td class="image-container">
                <img src="./research/C3Net_cvpr24.png" alt="C3Net">
            </td>
            <td class="publication-info">
                <h3>C3Net: Compound Conditioned ControlNet for Multimodal Content Generation, CVPR 2024</h3>
                Juntao Zhang, Yuehuai Liu, Yu-Wing Tai, Chi-Keung Tang<br>

                <FONT color=red>Multimodal (Text, Image, Audio) Content Generation from Multimodal Compound Conditioned Input</FONT><br>
                [<a href="https://arxiv.org/abs/2311.17951" target="_blank">Paper</a>] [<a href="https://github.com/JordanZh/C3Net" target="_blank">Project</a>] 
                
                <iframe src="https://ghbtns.com/github-btn.html?user=JordanZh&repo=C3Net&type=star&count=true&v=2" frameborder="0" scrolling="0" width="170" height="20"></iframe>
                
                </p>
            </td>
        </tr>
        <tr class="highlight">
            <td class="image-container">
                <img src="./research/hq-sam.gif" alt="HQ-SAM">
            </td>
            <td class="publication-info">
                <h3>Segment Anything in High Quality, NeurIPS 2023</h3>
                Lei Ke*, Mingqiao Ye*, Martin Danelljan, Yifan Liu, Yu-Wing Tai, Chi-Keung Tang, Fisher Yu
                (* denotes equal contribution)<br>

                <FONT color=red>HQ-SAM receives 2000+ Github stars in one month.</FONT><br>
                [<a href="https://arxiv.org/abs/2306.01567" target="_blank">Paper</a>] [<a href="https://github.com/SysCV/sam-hq" target="_blank">Project</a>]
                
                <iframe src="https://ghbtns.com/github-btn.html?user=SysCV&repo=sam-hq&type=star&count=true&v=2" frameborder="0" scrolling="0" width="170" height="20"></iframe>
                </p>
            </td>
        </tr>
        <tr class="highlight">
            <td class="image-container">
                <img src="./research/FaceDNeRF_NeurIPS2023.png" alt="FaceDNeRF">
            </td>
            <td class="publication-info">
                <h3>FaceDNeRF: Semantics-Driven Face Reconstruction, Prompt Editing and Relighting with Diffusion Models, NeurIPS 2023</h3>
                Hao Zhang*, Tianyuan Dai*, Yanbo Xu*, Yu-Wing Tai, Chi-Keung Tang
                (* denotes equal contribution)<br>

                <FONT color=red>3D faces from a single image with prompt editable and relighting ability.</FONT><br>
                [<a href="https://arxiv.org/abs/2306.00783" target="_blank">Paper</a>] [<a href="https://github.com/BillyXYB/FaceDNeRF" target="_blank">Project</a>][<a href="https://youtu.be/paxqlzW7z1Q" target="_blank">Youtube</a>]
                
                <iframe src="https://ghbtns.com/github-btn.html?user=BillyXYB&repo=FaceDNeRF&type=star&count=true&v=2" frameborder="0" scrolling="0" width="170" height="20"></iframe>
                </p>
            </td>
        </tr>
        <tr class="highlight">
            <td class="image-container">
                <img src="./research/bimatting_neurips23.png" alt="BiMatting">
            </td>
            <td class="publication-info">
                <h3>BiMatting: Efficient Video Matting via Binarization, NeurIPS 2023</h3>
                Haotong Qin*, Lei Ke*, Xudong Ma, Martin Danelljan, Yu-Wing Tai, Chi-Keung Tang, Xianglong Liu, Fisher Yu
                (* denotes equal contribution)<br>

                <FONT color=red>An accurate and efficient video matting model using binarization.</FONT><br>
                [<a href="https://openreview.net/pdf?id=YbYQ0JEQ80" target="_blank">Paper</a>] [<a href="https://github.com/htqin/BiMatting" target="_blank">Project</a>]
                
                <iframe src="https://ghbtns.com/github-btn.html?user=htqin&repo=BiMatting&type=star&count=true&v=2" frameborder="0" scrolling="0" width="170" height="20"></iframe>
                </p>
            </td>
        </tr>
        <tr class="highlight">
            <td class="image-container">
                <img src="./research/InstanceNeRF.png" alt="InstanceNeRF">
            </td>
            <td class="publication-info">
                <h3>Instance Neural Radiacne Field, ICCV 2023</h3>
                Yichen Liu*, Benran Hu*, Junkai Huang*, Yu-Wing Tai, Chi-Keung Tang
                (* denotes equal contribution)<br>

                <FONT color=red>This is the first instance segmentation framework for NeRF.</FONT><br>
                [<a href="https://arxiv.org/abs/2304.04395" target="_blank">Paper</a>] [<a href="https://github.com/lyclyc52/Instance_NeRF" target="_blank">Project</a>][<a href="https://youtu.be/wW9Bme73coI" target="_blank">Youtube</a>]
                
                <iframe src="https://ghbtns.com/github-btn.html?user=lyclyc52&repo=Instance_NeRF&type=star&count=true&v=2" frameborder="0" scrolling="0" width="170" height="20"></iframe>
                </p>
            </td>
        </tr>
        <tr class="highlight">
            <td class="image-container">
                <img src="./research/iccv23_cascade_detr.png" alt="CascadeDet">
            </td>
            <td class="publication-info">
                <h3>Cascade-DETR: Delving into High-Quality Universal Object Detection, ICCV 2023</h3>
                Mingqiao Ye*, Lei Ke*, Siyuan Li, Yu-Wing Tai, Chi-Keung Tang, Martin Danelljan, Fisher Yu
                (* denotes equal contribution)<br>

                <FONT color=red>Promoting DETR's detection accuracy in universal domains via cascade attention.</FONT><br>
                [<a href="https://arxiv.org/abs/2307.11035" target="_blank">Paper</a>] [<a href="https://github.com/SysCV/cascade-detr" target="_blank">Project</a>]
                
                <iframe src="https://ghbtns.com/github-btn.html?user=SysCV&repo=cascade-detr&type=star&count=true&v=2" frameborder="0" scrolling="0" width="170" height="20"></iframe>
                </p>
            </td>
        </tr>
        <tr class="highlight">
            <td class="image-container">
                <img src="./research/egohoi_iccv23.jpeg" alt="EgoHOI">
            </td>
            <td class="publication-info">
                <h3>EgoPCA: A New Framework for Egocentric Hand-Object Interaction Understanding, ICCV 2023</h3>
                Yue Xu, Yong-Lu Li, Zhemin Huang, Michael Xu LIU, Cewu Lu, Yu-Wing Tai, Chi Keung Tang
                <br>

                <FONT color=red>We contribute comprehensive pre-train sets, balanced test sets and a new baseline for Egocentric Hand-Object Interaction (Ego-HOI).</FONT><br>
                [<a href="https://arxiv.org/abs/2309.02423" target="_blank">Paper</a>] [<a href="https://github.com/silicx/EgoPCA" target="_blank">Project</a>]
                
                <iframe src="https://ghbtns.com/github-btn.html?user=silicx&repo=EgoPCA&type=star&count=true&v=2" frameborder="0" scrolling="0" width="170" height="20"></iframe>
                </p>
            </td>
        </tr>
        <tr class="highlight">
            <td class="image-container">
                <img src="./research/nerf-rpn.gif" alt="NeRF-RPN">
            </td>
            <td class="publication-info">
                <h3>NeRF-RPN: A general framework for object detection in NeRFs, CVPR 2023</h3>
                Benran Hu*, Junkai Huang*, Yichen Liu*, Yu-Wing Tai, Chi-Keung Tang
                (* denotes equal contribution)<br>

                <FONT color=red>This is the first object detection framework for NeRF.</FONT><br>
                [<a href="https://arxiv.org/abs/2211.11646" target="_blank">Paper</a>] [<a href="https://github.com/lyclyc52/NeRF_RPN" target="_blank">Project</a>][<a href="https://youtu.be/M8_4Ih1CJjE?si=Z8aIxhyWp4y5C-LO" target="_blank">Youtube</a>]
                
                <iframe src="https://ghbtns.com/github-btn.html?user=lyclyc52&repo=NeRF_RPN&type=star&count=true&v=2" frameborder="0" scrolling="0" width="170" height="20"></iframe>
                </p>
            </td>
        </tr>
        <tr class="highlight">
            <td class="image-container">
                <img src="./research/cvpr23_mfvis.png" alt="Mask-Free Video Instance Segmentation">
            </td>
            <td class="publication-info">
                <h3>Mask-Free Video Instance Segmentation, CVPR 2023</h3>
                Lei Ke, Martin Danelljan, Henghui Ding, Yu-Wing Tai, Chi-Keung Tang, Fisher Yu<br>

                <FONT color=red>Removing video and image mask annotation necessity for highly accurate VIS.</FONT><br>
                [<a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ke_Mask-Free_Video_Instance_Segmentation_CVPR_2023_paper.pdf" target="_blank">Paper</a>] 
                [<a href="https://github.com/SysCV/maskfreevis" target="_blank">Project</a>]
                <iframe src="https://ghbtns.com/github-btn.html?user=SysCV&repo=maskfreevis&type=star&count=true&v=2" frameborder="0" scrolling="0" width="170" height="20"></iframe>
            </td>
        </tr>
        <!-- Add more rows as needed -->
    </table>
</body>
</html>
